HW 5
================

``` r
library(tidyverse)
library(rvest)
library(lattice)
library("animation")
library("magrittr")

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() +  theme(legend.position = "bottom"))

options(
  ggplots2.continuous.color = "viridis",
  ggplots2.continuous.fill = "viridus"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Read in the data.

``` r
homicide_df = 
  read_csv("homicide-data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL") # data entry error =1, affter arrange we get rid of it 
```

Let’s look at this a bit.

``` r
aggregate_df =
homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

Can I do a prop test for a single city?

``` r
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved),
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy()
```

    ## # A tibble: 1 x 8
    ##   estimate statistic  p.value parameter conf.low conf.high method    alternative
    ##      <dbl>     <dbl>    <dbl>     <int>    <dbl>     <dbl> <chr>     <chr>      
    ## 1    0.646      239. 6.46e-54         1    0.628     0.663 1-sample… two.sided

Try to iterate…

``` r
results_df =
aggregate_df %>% 
  mutate(
    prop.tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)), # x is number of success and n is total number
    tidy_tests = map(.x = prop.tests, ~broom::tidy(.x))
    ) %>% 
 # %>% pull(prop.tests) delete tidy_tests in mutate then do this
  select(-prop.tests) %>% 
  unnest(tidy_tests) %>%  #show the test tibble
  select(city_state, estimate, conf.low, conf.high)
```

``` r
results_df %>% 
  mutate(city_state = fct_reorder(city_state,estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw5_mb4757_files/figure-gfm/unnamed-chunk-5-1.png" width="90%" />

``` r
city_prop_test = function(df){
  n_unsolved...
  n_total...
  prop.test(...)
} ##amother way to solve this problem

homicide_df = 
  read_csv("data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL") %>% # data entry error =1, affter arrange we get rid of it 
  nest(data = resolved)
```

    ## Error: 'data/homicide-data.csv' does not exist in current working directory ('/Users/Anna/Desktop/P8105 data science/p8105_hw5_mb4757').

## Problem 2

take a look at 1 csv file

``` r
data_1 = read_csv("lda_data/con_01.csv")
```

make the tibble and tidy the results

``` r
tidy_df =
tibble(
path = list.files("lda_data")
) %>% 
  mutate(
    path = str_c("lda_data/", path),
    data = purrr::map(.x = path, ~ read_csv(.x))) %>% 
    unnest(data) %>% 
  separate(path, into = c("path", "other"), sep= "/") %>% 
  separate(other, into = c("arm", "rest"), sep= "_") %>% 
  separate(rest, into = c("subject_ID", "remain"), sep= 2) %>% 
  select(-path, -remain) %>% 
    pivot_longer(
    week_1:week_8,
    names_to = "time",
    values_to = "observations"
    ) %>% 
  mutate(
    arm = str_replace(arm, "con", "control"),
    arm = str_replace(arm, "exp", "experimental")
  ) %>% 
  separate(time, into = c("name", "week"), sep= "_") %>% 
  select(-name)
  

control_arm_df = 
tidy_df %>% 
  filter(arm == "control") %>% 
  rename(control_arm = arm)
```

make the spaghetti plot

``` r
tidy_df %>% 
  group_by(arm) %>% 
ggplot(aes(x=week, y=observations, group = subject_ID, color=factor(subject_ID))) +
  geom_path(alpha = 0.7) + 
  geom_point(alpha = 0.7) +
  facet_grid(.~arm)
```

<img src="p8105_hw5_mb4757_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

``` r
# + scale_color_hue(name = "subject_ID") ## only for discrete data
```

In these two time-series plots, both control and experimental groups’
observations are oscillating. The trend of control group’s observations
are flat over time. However, as the time increases, the experimental
group’s observations are increasing.

## Problem 3

write a hypothesis function

``` r
hypothesis= function(sample_size = 30, mu, sigma = 5) { 
  sim_data = 
  tibble(
    x = rnorm(n = sample_size, mean = mu, sd = sigma)
  )
  
    t.test(sim_data, 
           alternative = "two.sided",
           mu = 0, paired = FALSE, 
           var.equal = FALSE, 
           conf.level = 0.95) %>% 
            broom::tidy() %>% 
      select(estimate, p.value)

}
```

function validation

``` r
hypothesis(30, 0, 5)
```

    ## # A tibble: 1 x 2
    ##   estimate p.value
    ##      <dbl>   <dbl>
    ## 1    -1.75  0.0364

Create 5000 datasets for hypothesis test mu = 0 iteration

``` r
output = vector("list", length = 5000)

for (i in 1:5000){
  output[[i]] = hypothesis(mu = 0)
} 

bind_rows(output)
```

    ## # A tibble: 5,000 x 2
    ##    estimate p.value
    ##       <dbl>   <dbl>
    ##  1    0.834  0.388 
    ##  2    0.466  0.636 
    ##  3   -0.268  0.758 
    ##  4    0.229  0.795 
    ##  5   -1.51   0.0555
    ##  6   -0.189  0.846 
    ##  7    1.24   0.224 
    ##  8   -0.445  0.604 
    ##  9    0.136  0.868 
    ## 10   -0.724  0.487 
    ## # … with 4,990 more rows

Create 5000 datasets for hypothesis t.test(mu = 0) with true mu = 0:6
iteration

``` r
sim_results =
  tibble(mu = c(0, 1, 2, 3, 4, 5, 6)) %>% 
  mutate(
    output_lists = map(.x = mu, ~rerun (5000, hypothesis(mu = .x))),
    estimate_df = map(output_lists, bind_rows)
    ) %>% 
  select(mu, estimate_df) %>% 
  unnest(estimate_df) %>% 
  rename(mu_hat = estimate)
```

make plots\!

``` r
plot1_df = 
sim_results %>% 
  mutate(
    decision_rule = case_when(
      p.value < 0.05  ~ "reject_null",
      p.value >= 0.05 ~ "fail_to_reject",
    )
  ) %>% 
  group_by(mu) %>% 
    count(decision_rule) %>% 
  filter(decision_rule == "reject_null") %>% 
  mutate(
    proportion = n/5000
  )

plot1_df %>% 
ggplot(aes(x=mu, y=proportion, group = 1, color = factor(mu))) +
  geom_path() + 
  geom_point() +
  labs(x = "true mean(mu)", 
       y = "proportion of reject_null", 
       title = "the power of the test vs true mean")
```

<img src="p8105_hw5_mb4757_files/figure-gfm/unnamed-chunk-14-1.png" width="90%" />

Overlay plots\!\!

``` r
plot2_df = 
sim_results %>% 
  mutate(
    decision_rule = case_when(
      p.value < 0.05  ~ "reject_null",
      p.value >= 0.05 ~ "fail_to_reject"
    )) %>% 
  group_by(mu) %>% 
  summarize(
    mean_mu_hat = mean(mu_hat)
  )
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

``` r
plot3_df = 
sim_results %>% 
  mutate(
    decision_rule = case_when(
      p.value < 0.05  ~ "reject_null",
      p.value >= 0.05 ~ "fail_to_reject"
    )) %>% 
  filter(decision_rule == "reject_null") %>% 
  group_by(mu) %>% 
  summarize(
    mean_mu_hat = mean(mu_hat)
  )
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

``` r
ggplot(plot2_df, aes(x = mu, y = mean_mu_hat)) +
  geom_path(color = "red") + 
  geom_point(color = "red") +
  geom_path(plot3_df, mapping = aes(x = mu, y = mean_mu_hat)) + 
  geom_point(plot3_df, mapping = aes(x = mu, y = mean_mu_hat)) +
  labs(x = "true mean(mu)", 
       y = "average estimate of mu_hat ", 
       title = "average estimate of mu_hat vs true mean")   
```

<img src="p8105_hw5_mb4757_files/figure-gfm/unnamed-chunk-15-1.png" width="90%" />

``` r
#The red line is the average estimate of mu_hat vs the true value of true mean.

#The black line is the average estimate of mu_hat only in samples for which the null was rejected vs the true mean.
```

When true mean mu is from 0 to 3, the sample average of mu\_hat across
tests for which the null is rejected not approximately equal to the true
value of mu. True mean(mu) and mean\_mu\_hat are different from each
other because power is small. This means Type II error is big since
power = 1-beta(Type II error).

When true mean mu is from 4 to 6, the sample average of mu\_hat across
tests for which the null is rejected approximately equal to the true
value of mu.

As the true mean mu increase, the proportion of times the null was
rejected is higher and true mean(mu) almost equal to mean\_mu\_hat. This
happens because power is large, which means type II error is small.

We can see how does the power changes as the true mean(mu) increase from
the following table.

``` r
power_table =
plot1_df %>% 
  rename(power = proportion)
knitr::kable(power_table, title = "power table for mu from 0 to 6")
```

| mu | decision\_rule |    n |  power |
| -: | :------------- | ---: | -----: |
|  0 | reject\_null   |  276 | 0.0552 |
|  1 | reject\_null   |  898 | 0.1796 |
|  2 | reject\_null   | 2870 | 0.5740 |
|  3 | reject\_null   | 4417 | 0.8834 |
|  4 | reject\_null   | 4945 | 0.9890 |
|  5 | reject\_null   | 4997 | 0.9994 |
|  6 | reject\_null   | 5000 | 1.0000 |

``` r
#notes:
#Power = 1 – beta(type II error). Significance level = 1 - alpha(Type I error). In reality, we want both Type I and Type II errors to be small. In terms of significance level and power, which means we want a larger significance level and a large power. 

#Type 2 error (failing to reject a false null hypothesis) can be minimized either by picking a larger sample size or by choosing a "threshold" alternative value of the parameter in question that is further from the null value.
```
